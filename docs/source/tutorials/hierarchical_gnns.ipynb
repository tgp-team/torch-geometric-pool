{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# Hierarchical Graph Neural Networks with [<img src=\"https://raw.githubusercontent.com/tgp-team/torch-geometric-pool/main/docs/source/_static/img/tgp-logo.svg\" width=\"20px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp](https://github.com/tgp-team/torch-geometric-pool)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tgp-team/torch-geometric-pool/blob/main/docs/source/tutorials/hierarchical_gnns.ipynb)\n",
    "\n",
    "Graph pooling is a fundamental operation in Graph Neural Networks (GNNs) that enables **hierarchical learning** by coarsening graphs into smaller representations. Just as convolutional neural networks use pooling to build hierarchical features in images, graph pooling allows GNNs to capture multi-scale patterns in graph-structured data.\n",
    "\n",
    "In this tutorial, we'll explore [<img src=\"https://raw.githubusercontent.com/tgp-team/torch-geometric-pool/main/docs/source/_static/img/tgp-logo.svg\" width=\"20px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp](https://github.com/tgp-team/torch-geometric-pool) (Torch Geometric Pool), a comprehensive library that provides a unified framework for graph pooling operators. We'll learn how to:\n",
    "\n",
    "- Understand the **SRC framework** ($\\texttt{SEL}$-$\\texttt{RED}$-$\\texttt{CON}$) that unifies all pooling operators\n",
    "- **Visualize** pooling operations to understand what they do\n",
    "- Build GNNs with and without pooling to understand the benefits\n",
    "- Work with both **sparse** and **dense** poolers\n",
    "- Create **hierarchical architectures** with multiple pooling layers\n",
    "- Optimize performance with **caching** and **precoarsening**\n",
    "- Create **custom pooling operators** by mixing and matching components\n",
    "\n",
    "For more details, see the [tgp paper](https://arxiv.org/abs/2512.12642) and the [documentation](https://torch-geometric-pool.readthedocs.io).\n",
    "\n",
    "\n",
    "**This is the exercise version** of the tutorial. The full step-by-step solution for the custom pooler exercise is in the [Advanced](https://github.com/tgp-team/torch-geometric-pool/blob/main/docs/source/tutorials/advanced.ipynb) notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required libraries. If you're running this in Google Colab, you'll need to restart the session after running the installation cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "installation-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    %pip install torch==2.4.1 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "installation-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    %pip install torch_geometric==2.6.1\n",
    "    %pip install torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
    "    %pip install pygsp==0.6.1\n",
    "    %pip install -q git+https://github.com/tgp-team/torch-geometric-pool.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "Now let's import the libraries we'll need throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, DenseGCNConv, global_mean_pool\n",
    "from torch_geometric import seed_everything\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tgp.datasets import PyGSPDataset\n",
    "\n",
    "from tgp.poolers import get_pooler, pooler_classes, pooler_map\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_everything(42)\n",
    "torch.set_printoptions(threshold=2, edgeitems=2)\n",
    "\n",
    "# Load MUTAG dataset (we'll use this throughout the tutorial)\n",
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "src-framework-header",
   "metadata": {},
   "source": [
    "## The SRC Framework\n",
    "\n",
    "All pooling operators in [<img src=\"https://raw.githubusercontent.com/tgp-team/torch-geometric-pool/main/docs/source/_static/img/tgp-logo.svg\" width=\"20px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp](https://github.com/tgp-team/torch-geometric-pool) follow the **SRC framework**, which decomposes pooling into three fundamental operations:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tgp-team/torch-geometric-pool/main/docs/source/_static/img/src_overview.png\" style=\"width: 55%; display: block; margin: auto;\">\n",
    "\n",
    "- $\\texttt{SEL}$ (**SELECT**): Determines how nodes map to supernodes in the coarsened graph. This produces a [`SelectOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.SelectOutput) containing the assignment matrix $\\mathbf{S}$.\n",
    "- $\\texttt{RED}$ (**REDUCE**): Aggregates features of nodes assigned to the same supernode using $\\mathbf{S}$.\n",
    "- $\\texttt{CON}$ (**CONNECT**): Creates edges between supernodes in the coarsened graph using $\\mathbf{S}$.\n",
    "\n",
    "There's also a $\\texttt{LIFT}$ operation for unpooling (mapping coarsened features back to the original graph), which is useful for tasks like node classification.\n",
    "\n",
    "This abstraction makes it easy to understand, compare, and create custom pooling operators. For more details, see the [SRC paper](https://arxiv.org/abs/2110.05292).\n",
    "\n",
    "Let's explore the available poolers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-poolers",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available pooling operators in tgp:\")\n",
    "for i, pooler in enumerate(pooler_classes):\n",
    "    print(f\"{i+1:2d}. {pooler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aliases-header",
   "metadata": {},
   "source": [
    "Each pooler has a convenient alias for quick instantiation using [`get_pooler`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.get_pooler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-aliases",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pooler aliases:\")\n",
    "for alias, cls in pooler_map.items():\n",
    "    print(f\"  '{alias:8s}' → {cls.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-pooler-header",
   "metadata": {},
   "source": [
    "Let's create a [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) operator and examine its SRC components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-pooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tgp.poolers import TopkPooling\n",
    "\n",
    "# Create a TopK pooler\n",
    "pooler = TopkPooling(in_channels=7, ratio=0.5)\n",
    "print(f\"Pooler structure:\")\n",
    "print(pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pooling-output-header",
   "metadata": {},
   "source": [
    "### PoolingOutput and SelectOutput\n",
    "\n",
    "When we apply a pooler, it returns a [`PoolingOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/src.html#tgp.src.PoolingOutput) object containing:\n",
    "- `x`: Node features of the coarsened graph\n",
    "- `edge_index`: Edge connectivity of the coarsened graph\n",
    "- `edge_weight`: Edge weights (if computed)\n",
    "- `batch`: Batch assignment for the coarsened nodes\n",
    "- `so`: [`SelectOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.SelectOutput) - describes the node-to-supernode mapping\n",
    "- `loss`: Auxiliary losses (if any)\n",
    "\n",
    "Let's apply the pooler to a batch of graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-pooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small batch from the dataset\n",
    "batch = Batch.from_data_list(dataset[:5])\n",
    "print(f\"Input batch: {batch}\")\n",
    "print(f\"  Nodes: {batch.x.shape[0]}\")\n",
    "print(f\"  Edges: {batch.edge_index.shape[1]}\")\n",
    "\n",
    "# Apply pooling\n",
    "pool_out = pooler(x=batch.x, adj=batch.edge_index, batch=batch.batch)\n",
    "print(f\"\\nPooling output: {pool_out}\")\n",
    "print(f\"  Pooled nodes: {pool_out.x.shape[0]}\")\n",
    "print(f\"  Pooled edges: {pool_out.edge_index.shape[1]}\")\n",
    "print(f\"\\nGraph was coarsened by ratio: {pool_out.x.shape[0] / batch.x.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine-select-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SelectOutput:\")\n",
    "print(pool_out.so)\n",
    "print(f\"\\nIt maps {pool_out.so.num_nodes} nodes to {pool_out.so.num_supernodes} supernodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lift-header",
   "metadata": {},
   "source": [
    "### The LIFT Operation\n",
    "\n",
    "Pooling can also be reversed! The $\\texttt{LIFT}$ operation maps coarsened features back to the original node space. This is useful for node-level tasks like node classification.\n",
    "\n",
    "See the [node classification example](https://github.com/tgp-team/torch-geometric-pool/blob/main/examples/classification_node.py) for a complete use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrate-lift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift the pooled features back to original space\n",
    "x_lifted = pooler(x=pool_out.x, so=pool_out.so, batch=pool_out.batch, lifting=True)\n",
    "\n",
    "print(f\"Original features shape: {batch.x.shape}\")\n",
    "print(f\"Pooled features shape: {pool_out.x.shape}\")\n",
    "print(f\"Lifted features shape: {x_lifted.shape}\")\n",
    "print(\"\\nLifting successfully maps pooled features back to original node space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis-header",
   "metadata": {},
   "source": [
    "## Visualizing Pooling Operations\n",
    " \n",
    "Understanding how pooling works is easier when we can visualize it. Let's create some visualizations of the SELECT operation and the resulting graph coarsening.\n",
    " \n",
    "The modular and standardized API of the poolers in [<img src=\"https://raw.githubusercontent.com/tgp-team/torch-geometric-pool/main/docs/source/_static/img/tgp-logo.svg\" width=\"20px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp](https://github.com/tgp-team/torch-geometric-pool) allows for using the same functions to visualize the pooling results.\n",
    "For testing and debugging, we can rely on the simple point clouds provided by the [PyGSP](https://pygsp.readthedocs.io/en/stable/) library for which [tgp](https://github.com/tgp-team/torch-geometric-pool) provide a wrapper for converting them into a proper torch dataset.\n",
    "Below, we create a batch composed of a 2D-Grid, a Ring, and a Community graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pygsp",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = PyGSPDataset(\n",
    "    root=\"/tmp/PyGSP/\",\n",
    "    name=\"Grid2d\",\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    pre_filter=None,\n",
    "    force_reload=True,\n",
    "    kwargs={\"N1\": 5, \"N2\": 5},\n",
    ")[0]\n",
    "print(f\"2D-Grid: {grid}\")\n",
    "\n",
    "ring = PyGSPDataset(\n",
    "    root=\"/tmp/PyGSP/\",\n",
    "    name=\"Ring\",\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    pre_filter=None,\n",
    "    force_reload=True,\n",
    "    kwargs={\"N\": 30},\n",
    ")[0]\n",
    "ring.update({\"x\": ring.x + 2.0})\n",
    "print(f\"Ring: {ring}\")\n",
    "\n",
    "community = PyGSPDataset(\n",
    "    root=\"/tmp/PyGSP/\",\n",
    "    name=\"Community\",\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    pre_filter=None,\n",
    "    force_reload=True,\n",
    "    kwargs={\"N\": 18, \"Nc\": 3},\n",
    ")[0]\n",
    "community.update(\n",
    "    {\"x\": torch.stack([community.x[:, 0] * 0.2 + 4.0, community.x[:, 1] * 0.2], dim=1)}\n",
    ")\n",
    "print(f\"Community: {community}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-pygsp-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch from the three datasets\n",
    "pygsp_batch = Batch.from_data_list([grid, ring, community])\n",
    "print(pygsp_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s-matrix-header",
   "metadata": {},
   "source": [
    "### Visualizing the S Matrix\n",
    "\n",
    "The SELECT operation produces an assignment matrix **S** that maps nodes to supernodes. Let's visualize this matrix for different poolers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s-matrix-topk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create poolers\n",
    "topk_pooler = TopkPooling(in_channels=2, ratio=0.5)\n",
    "\n",
    "# Get SelectOutput\n",
    "so_topk = topk_pooler.select(x=pygsp_batch.x, batch=pygsp_batch.batch)\n",
    "\n",
    "# Visualize S matrix\n",
    "plt.figure(figsize=(4, 5))\n",
    "plt.imshow(so_topk.s.to_dense().detach().numpy(), cmap='viridis', aspect='auto')\n",
    "plt.title(r\"$\\mathbf{S}$ Matrix (TopK Pooling)\", fontsize=14)\n",
    "plt.xlabel(\"Supernodes\", fontsize=12)\n",
    "plt.ylabel(\"Original Nodes\", fontsize=12)\n",
    "plt.colorbar(label='Assignment Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"S matrix shape: [{so_topk.num_nodes}, {so_topk.num_supernodes}]\")\n",
    "print(f\"Sparse assignment: {not so_topk.is_expressive}\")\n",
    "print(\"Each node is assigned to exactly one supernode (sparse pooling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s-matrix-mincut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinCut pooler (dense assignment)\n",
    "mincut_vis = get_pooler('mincut', in_channels=2, k=20)\n",
    "\n",
    "# Apply pooler and get SelectOutput for visualization\n",
    "pool_out_mincut = mincut_vis(x=pygsp_batch.x, adj=pygsp_batch.edge_index, batch=pygsp_batch.batch)\n",
    "so_mincut = pool_out_mincut.so\n",
    "\n",
    "# Visualize S matrix\n",
    "plt.figure(figsize=(4, 5))\n",
    "S_matrix = so_mincut.s[0].detach().numpy()  # First graph in batch\n",
    "plt.imshow(S_matrix, cmap='viridis', aspect='auto')\n",
    "plt.title(r\"$\\mathbf{S}$ Matrix (MinCut Pooling) - First Graph\", fontsize=14)\n",
    "plt.xlabel(\"Supernodes\", fontsize=12)\n",
    "plt.ylabel(\"Original Nodes\", fontsize=12)\n",
    "plt.colorbar(label='Assignment Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"S matrix shape: {S_matrix.shape}\")\n",
    "print(f\"Soft assignment: {so_mincut.is_expressive}\")\n",
    "print(\"Nodes can belong to multiple supernodes with different weights (dense pooling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-coarsening-header",
   "metadata": {},
   "source": [
    "### Visualizing Graph Coarsening\n",
    "\n",
    "Now let's visualize how pooling coarsens the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vis-original-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original graph\n",
    "G = to_networkx(pygsp_batch, to_undirected=True)\n",
    "pos = pygsp_batch.x.numpy()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "nx.draw(G, pos=pos, node_size=20, node_color='lightblue', \n",
    "        edge_color='gray', with_labels=False, alpha=0.8)\n",
    "plt.title(\"Original Graphs (3 graphs batched)\", fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total nodes: {pygsp_batch.x.shape[0]}\")\n",
    "print(f\"Total edges: {pygsp_batch.edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vis-selected-nodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pooling and highlight selected nodes\n",
    "pool_out = topk_pooler(x=pygsp_batch.x, adj=pygsp_batch.edge_index, batch=pygsp_batch.batch)\n",
    "\n",
    "# Get indices of selected nodes\n",
    "selected_indices = pool_out.so.s.indices()[0].numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "# Draw all nodes in gray\n",
    "nx.draw(G, pos=pos, node_size=20, node_color='lightgray', \n",
    "        edge_color='lightgray', with_labels=False, alpha=0.5)\n",
    "# Highlight selected nodes in red\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=selected_indices.tolist(),\n",
    "                       node_color='red', node_size=20, alpha=0.9)\n",
    "plt.title(f\"After TopK Pooling (red nodes selected, {len(selected_indices)}/{pygsp_batch.x.shape[0]} nodes kept)\", \n",
    "          fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Selected nodes: {len(selected_indices)} out of {pygsp_batch.x.shape[0]}\")\n",
    "print(f\"Reduction ratio: {len(selected_indices)/pygsp_batch.x.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vis-coarsened-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the coarsened graph structure\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create Data object for pooled graph\n",
    "pooled_data = Data(x=pool_out.x, edge_index=pool_out.edge_index, batch=pool_out.batch)\n",
    "G_pooled = to_networkx(pooled_data, to_undirected=True)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "# Use pooled features as positions (which are the original positions of selected nodes)\n",
    "pos_pooled = pool_out.x.detach().numpy()\n",
    "nx.draw_networkx_nodes(G_pooled, pos=pos_pooled, node_size=20, \n",
    "                       node_color=pool_out.batch.numpy(), \n",
    "                       cmap='Set3', alpha=0.8)\n",
    "nx.draw_networkx_edges(G_pooled, pos=pos_pooled, edge_color='gray', \n",
    "                       alpha=0.5, width=2)\n",
    "plt.title(\"Coarsened Graph Structure\", fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coarsened graph nodes: {pool_out.x.shape[0]}\")\n",
    "print(f\"Coarsened graph edges: {pool_out.edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "We'll use the **MUTAG** dataset for graph classification tasks. This dataset contains 188 chemical compounds (molecules), where each graph represents a molecule and the task is to predict whether it has mutagenic effects on a bacterium (binary classification).\n",
    "\n",
    "For more information about datasets in PyG, see the [PyG documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(f'Dataset: {dataset}')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# Create train and test loaders\n",
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Look at one batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f'\\nSample batch: {batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## Baseline GNN without Pooling\n",
    "\n",
    "Let's start by building a simple GNN **without** graph pooling. This will serve as our baseline to understand the benefits that pooling brings.\n",
    "\n",
    "Our baseline architecture uses [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html) layers followed by global mean pooling:\n",
    "\n",
    "```\n",
    "[GCNConv → ReLU → GCNConv → ReLU → GlobalMeanPool → Linear]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(BaselineGCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Two GCN layers\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings through GCN layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # 2. Readout layer: aggregate node features to graph-level\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "baseline_model = BaselineGCN(hidden_channels=64)\n",
    "print(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-train-header",
   "metadata": {},
   "source": [
    "Let's train the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = baseline_model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "print(\"Training baseline GNN (no pooling)...\")\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'\\nFinal Test Accuracy (Baseline): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pooling-intro-header",
   "metadata": {},
   "source": [
    "## Adding Pooling to a GNN\n",
    "\n",
    "Now let's see how easy it is to add graph pooling to our GNN! We simply insert a pooling layer between message-passing layers.\n",
    "\n",
    "Our new architecture with [`MaxCutPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.MaxCutPooling):\n",
    "\n",
    "```\n",
    "[GCNConv → MaxCutPool → GCNConv → GlobalPool → Linear]\n",
    "```\n",
    "\n",
    "Notice how easy it is - we just add one line with `get_pooler()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnn-with-pooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithPooling(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First GCN layer\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        \n",
    "        # Pooling layer - easily added with get_pooler!\n",
    "        self.pooler = get_pooler('maxcut', in_channels=hidden_channels, ratio=0.5)\n",
    "        \n",
    "        # Second GCN layer (after pooling)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Classifier\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight, batch):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pooling: coarsen the graph\n",
    "        pool_out = self.pooler(x=x, adj=edge_index, edge_weight=edge_weight, batch=batch)\n",
    "        x_pooled = pool_out.x\n",
    "        edge_index_pooled = pool_out.edge_index\n",
    "        edge_weight_pooled = pool_out.edge_weight\n",
    "        batch_pooled = pool_out.batch\n",
    "        \n",
    "        # Second GCN layer on the pooled graph\n",
    "        x = self.conv2(x_pooled, edge_index_pooled, edge_weight_pooled)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.pooler.global_pool(x, reduce_op='mean', batch=batch_pooled)\n",
    "        \n",
    "        # Classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "pooling_model = GNNWithPooling(hidden_channels=64)\n",
    "print(pooling_model)\n",
    "print(f\"\\nPooler details:\")\n",
    "print(pooling_model.pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-with-pooling-header",
   "metadata": {},
   "source": [
    "Notice how the pooler shows its internal SRC components: `select`, `reduce`, `lift`, and `connect`. This is the SRC framework in action!\n",
    "\n",
    "Let's train this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-with-pooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pooling_model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "print(\"Training GNN with pooling...\")\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'\\nFinal Test Accuracy (with pooling): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-poolers-header",
   "metadata": {},
   "source": [
    "## Dense Poolers and Auxiliary Losses\n",
    "\n",
    "tgp supports two types of pooling operators:\n",
    "- **Sparse poolers**: Work with edge lists (like TopK, SAG, ASAP)\n",
    "- **Dense poolers**: Work with dense adjacency matrices (like MinCut, DiffPool, DMoN)\n",
    "\n",
    "Dense poolers often come with **auxiliary losses** that help guide the learning process (e.g., the cut loss and orthogonality loss in [`MinCutPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.MinCutPooling)).\n",
    "\n",
    "Let's explore MinCut pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dense-pooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinCut pooler (dense)\n",
    "dense_pooler = get_pooler('mincut', in_channels=7, k=20)\n",
    "print(\"MinCut pooler (dense):\")\n",
    "print(dense_pooler)\n",
    "print(f\"\\nIs dense? {dense_pooler.is_dense}\")\n",
    "print(f\"Has auxiliary loss? Check the cut_loss_coeff and ortho_loss_coeff parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-preprocessing-header",
   "metadata": {},
   "source": [
    "Poolers accept \\((x, adj, batch)\\) directly; dense poolers handle conversion internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply-dense-pooler-header",
   "metadata": {},
   "source": [
    "Let's apply the dense pooler and examine the auxiliary losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-dense-pooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dense pooling\n",
    "dense_pool_out = dense_pooler(x=batch.x, adj=batch.edge_index, batch=batch.batch)\n",
    "\n",
    "print(\"Dense pooling output:\")\n",
    "print(dense_pool_out)\n",
    "print(f\"\\nPooled graph has {dense_pool_out.so.num_supernodes} supernodes (as specified)\")\n",
    "print(f\"\\nAuxiliary losses:\")\n",
    "for loss_name, loss_value in dense_pool_out.loss.items():\n",
    "    print(f\"  {loss_name}: {loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gnn-dense-pooling-header",
   "metadata": {},
   "source": [
    "### Building a GNN with Dense Pooling\n",
    "\n",
    "When using dense poolers, we need to:\n",
    "1. Use [`DenseGCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.DenseGCNConv.html) layers after pooling (instead of regular `GCNConv`)\n",
    "2. Add auxiliary losses to the total loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnn-dense-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithDensePooling(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First GCN layer (sparse)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        \n",
    "        # Dense pooling layer\n",
    "        self.pooler = get_pooler('mincut', in_channels=hidden_channels, k=20, cut_loss_coeff=0.01, ortho_loss_coeff=0.01)\n",
    "        \n",
    "        # Second GCN layer (dense, because pooler is dense)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Classifier\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight, batch):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Dense pooling\n",
    "        pool_out = self.pooler(x=x, adj=edge_index, edge_weight=edge_weight, batch=batch)\n",
    "        \n",
    "        # Second GCN layer (on dense adjacency)\n",
    "        x = self.conv2(pool_out.x, pool_out.edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.pooler.global_pool(x, reduce_op='sum', batch=None)  # batch=None for dense\n",
    "        \n",
    "        # Classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # Return predictions and auxiliary loss\n",
    "        aux_loss = sum(pool_out.get_loss_value()) if pool_out.loss else 0.0\n",
    "        return x, aux_loss\n",
    "\n",
    "dense_model = GNNWithDensePooling(hidden_channels=64)\n",
    "print(dense_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-dense-header",
   "metadata": {},
   "source": [
    "Training with auxiliary losses - note how we add `aux_loss` to the total loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-dense",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dense_model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_with_aux_loss():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, aux_loss = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "        loss = criterion(out, data.y) + aux_loss  # Add auxiliary loss!\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_with_aux(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out, _ = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "print(\"Training GNN with dense pooling and auxiliary losses...\")\n",
    "for epoch in range(1, 21):\n",
    "    loss = train_with_aux_loss()\n",
    "    train_acc = test_with_aux(train_loader)\n",
    "    test_acc = test_with_aux(test_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'\\nFinal Test Accuracy (Dense Pooling): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-flexibility-header",
   "metadata": {},
   "source": [
    "## Dense Poolers: Input/Output Flexibility\n",
    "\n",
    "Dense poolers in tgp are highly flexible. They support different configurations via the `batched` and `sparse_output` flags:\n",
    "\n",
    "| Configuration | `batched` | `sparse_output` | Input | Output |\n",
    "|---------------|-----------|-----------------|-------|--------|\n",
    "| Batched Dense | `True` (default) | `False` (default) | Sparse edge list | Dense adjacency |\n",
    "| Batched Sparse | `True` | `True` | Sparse edge list | Sparse edge list |\n",
    "| Unbatched Dense | `False` | `False` | Sparse edge list | Dense adjacency |\n",
    "| Unbatched Sparse | `False` | `True` | Sparse edge list | Sparse edge list |\n",
    "\n",
    "For details on when to use each mode and the trade-offs involved, see the [tgp paper](https://arxiv.org/abs/2512.12642)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4 MinCut configurations\n",
    "configs = [\n",
    "    ('Batched Dense', dict(batched=True, sparse_output=False)),\n",
    "    ('Batched Sparse', dict(batched=True, sparse_output=True)),\n",
    "    ('Unbatched Dense', dict(batched=False, sparse_output=False)),\n",
    "    ('Unbatched Sparse', dict(batched=False, sparse_output=True)),\n",
    "]\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for name, config in configs:\n",
    "    pooler = get_pooler('mincut', in_channels=7, k=10, **config)\n",
    "    out = pooler(x=batch.x, adj=batch.edge_index, batch=batch.batch)\n",
    "    \n",
    "    # Get output shapes\n",
    "    x_shape = out.x.shape\n",
    "    adj_shape = out.edge_index.shape if out.edge_index.dim() == 2 else f\"{out.edge_index.shape} (dense)\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  pooler.batched = {pooler.batched}\")\n",
    "    print(f\"  pooler.sparse_output = {pooler.sparse_output}\")\n",
    "    print(f\"  Output x shape: {x_shape}\")\n",
    "    print(f\"  Output edge_index shape: {adj_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-tradeoffs",
   "metadata": {},
   "source": [
    "**Trade-offs:**\n",
    "\n",
    "- **Batched mode** (`batched=True`): Efficient for batches of small graphs, but requires padding to handle variable-sized graphs\n",
    "- **Unbatched mode** (`batched=False`): Processes graphs one at a time, avoiding padding overhead for large graphs\n",
    "- **Dense output** (`sparse_output=False`): Required if using dense layers (like `DenseGCNConv`) after pooling\n",
    "- **Sparse output** (`sparse_output=True`): Allows using regular sparse layers after pooling, reducing memory for large graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical-header",
   "metadata": {},
   "source": [
    "## Hierarchical Architectures\n",
    "\n",
    "One of the key benefits of graph pooling is building truly **hierarchical architectures** with multiple pooling layers. This allows the network to learn features at different scales.\n",
    "\n",
    "Architecture with 2 pooling layers:\n",
    "```\n",
    "[GCN → Pool₁ → GCN → Pool₂ → GCN → GlobalPool → Linear]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hierarchical-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First GCN layer\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        \n",
    "        # First pooling layer (keep 50% of nodes)\n",
    "        self.pooler1 = get_pooler('topk', in_channels=hidden_channels, ratio=0.5)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Second pooling layer (keep 50% of remaining nodes)\n",
    "        self.pooler2 = get_pooler('topk', in_channels=hidden_channels, ratio=0.5)\n",
    "        \n",
    "        # Third GCN layer\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Classifier\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight, batch):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # First pooling\n",
    "        out1 = self.pooler1(x=x, adj=edge_index, edge_weight=edge_weight, batch=batch)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(out1.x, out1.edge_index, out1.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second pooling\n",
    "        out2 = self.pooler2(x=x, adj=out1.edge_index, edge_weight=out1.edge_weight, batch=out1.batch)\n",
    "        \n",
    "        # Third GCN layer\n",
    "        x = self.conv3(out2.x, out2.edge_index, out2.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.pooler2.global_pool(x, reduce_op='sum', batch=out2.batch)\n",
    "        \n",
    "        # Classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # Combine auxiliary losses from both poolers (if any)\n",
    "        total_aux_loss = 0.0\n",
    "        if out1.loss:\n",
    "            total_aux_loss += sum(out1.get_loss_value())\n",
    "        if out2.loss:\n",
    "            total_aux_loss += sum(out2.get_loss_value())\n",
    "        \n",
    "        return x, total_aux_loss\n",
    "\n",
    "hierarchical_model = HierarchicalGNN(hidden_channels=64)\n",
    "print(hierarchical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-hierarchical-header",
   "metadata": {},
   "source": [
    "Let's train the hierarchical model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-hierarchical",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hierarchical_model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training hierarchical GNN (2 pooling layers)...\")\n",
    "for epoch in range(1, 21):\n",
    "    loss = train_with_aux_loss()\n",
    "    train_acc = test_with_aux(train_loader)\n",
    "    test_acc = test_with_aux(test_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'\\nFinal Test Accuracy (Hierarchical with 2 pooling layers): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caching-header",
   "metadata": {},
   "source": [
    "## Caching for Single-Graph Tasks\n",
    "\n",
    "For tasks involving a **single large graph** (like node classification), the pooler is called with \\((x, adj)\\) and the decoder can use the original graph's adjacency.\n",
    "\n",
    "Let's switch to the **Cora** dataset, a citation network where we classify research papers into topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-cora",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "cora_dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "cora_data = cora_dataset[0]\n",
    "\n",
    "print(f'Dataset: {cora_dataset}')\n",
    "print(f'Data: {cora_data}')\n",
    "print(f'Number of nodes: {cora_data.num_nodes}')\n",
    "print(f'Number of edges: {cora_data.num_edges}')\n",
    "print(f'Number of features: {cora_dataset.num_features}')\n",
    "print(f'Number of classes: {cora_dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caching-explanation",
   "metadata": {},
   "source": [
    "For node classification, we often use an **encoder-decoder architecture** with pooling and $\\texttt{LIFT}$ in the middle:\n",
    "\n",
    "```\n",
    "[Encoder → Pool → Bottleneck → LIFT → Decoder]\n",
    "```\n",
    "\n",
    "The pooler is called with \\((x, adj, batch)\\) directly; for the decoder we use the original graph's adjacency (e.g. via `to_dense_adj`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "node-classification-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassificationGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv_enc = GCNConv(cora_dataset.num_features, hidden_channels)\n",
    "        \n",
    "        # Pooler\n",
    "        self.pooler = get_pooler(\n",
    "            'mincut',\n",
    "            in_channels=hidden_channels,\n",
    "            k=cora_data.num_nodes // 20,  # Coarsen to ~5% of original size\n",
    "            sparse_output=False\n",
    "        )\n",
    "        \n",
    "        # Bottleneck (on pooled graph)\n",
    "        self.conv_pool = DenseGCNConv(hidden_channels, hidden_channels // 2)\n",
    "        \n",
    "        # Decoder (after lifting)\n",
    "        self.conv_dec = DenseGCNConv(hidden_channels // 2, cora_dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        # Encoder\n",
    "        x = self.conv_enc(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # Pool\n",
    "        pool_out = self.pooler(x=x, adj=edge_index, edge_weight=edge_weight, batch=None)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x_pool = self.conv_pool(pool_out.x, pool_out.edge_index)\n",
    "        x_pool = F.relu(x_pool)\n",
    "        x_pool = F.dropout(x_pool, p=0.5, training=self.training)\n",
    "        \n",
    "        # Lift back to original space\n",
    "        x_lift = self.pooler(x=x_pool, so=pool_out.so, lifting=True, batch=None)\n",
    "        \n",
    "        # Decoder (dense adj for single graph)\n",
    "        adj_dense = to_dense_adj(edge_index)\n",
    "        x = self.conv_dec(x_lift, adj_dense)\n",
    "        \n",
    "        # Extract from batch dimension\n",
    "        if x.dim() == 3:\n",
    "            x = x[0]\n",
    "        \n",
    "        # Return predictions and auxiliary loss\n",
    "        aux_loss = sum(pool_out.get_loss_value()) if pool_out.loss else 0.0\n",
    "        return F.log_softmax(x, dim=-1), aux_loss\n",
    "\n",
    "node_model = NodeClassificationGNN(hidden_channels=16)\n",
    "print(node_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-node-classification-header",
   "metadata": {},
   "source": [
    "Training for node classification (we only train on labeled nodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-node-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node_model.to(device)\n",
    "cora_data = cora_data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train_node():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out, aux_loss = model(cora_data.x, cora_data.edge_index, cora_data.edge_weight)\n",
    "    loss = F.nll_loss(out[cora_data.train_mask], cora_data.y[cora_data.train_mask]) + aux_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_node():\n",
    "    model.eval()\n",
    "    out, _ = model(cora_data.x, cora_data.edge_index, cora_data.edge_weight)\n",
    "    pred = out.argmax(dim=1)\n",
    "    \n",
    "    accs = []\n",
    "    for mask in [cora_data.train_mask, cora_data.val_mask, cora_data.test_mask]:\n",
    "        correct = pred[mask].eq(cora_data.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "print(\"Training node classification with caching...\")\n",
    "for epoch in range(1, 201):\n",
    "    loss = train_node()\n",
    "    train_acc, val_acc, test_acc = test_node()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "print(f'\\nFinal Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precoarsening-header",
   "metadata": {},
   "source": [
    "## Precoarsening for Non-Learnable Poolers\n",
    "\n",
    "Some pooling operators are **non-learnable** - they compute node assignments based solely on the graph structure, not on learned node features. Examples include:\n",
    "- [`NDPPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.NDPPooling) (Node Decimation Pooling)\n",
    "- [`GraclusPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.GraclusPooling)\n",
    "- [`NMFPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.NMFPooling)\n",
    "\n",
    "Since the graph structure doesn't change during training, we can **precompute** the pooling operations using [`PreCoarsening`](https://torch-geometric-pool.readthedocs.io/en/latest/api/data/transforms.html#tgp.data.transforms.PreCoarsening)!\n",
    "\n",
    "For advanced precoarsening usages (multiple poolers, different configs per level), see the [Precoarsening and transforms](https://github.com/tgp-team/torch-geometric-pool/blob/main/docs/source/tutorials/precoarsening_and_transforms.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precoarsening-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tgp.data import PreCoarsening, PoolDataLoader\n",
    "from tgp.reduce import BaseReduce, global_reduce\n",
    "from torch_geometric.nn import ARMAConv\n",
    "\n",
    "# Create a non-learnable pooler\n",
    "ndp_pooler = get_pooler('ndp')  # Node Decimation Pooling\n",
    "print(f\"NDP Pooler: {ndp_pooler}\")\n",
    "\n",
    "# Apply PreCoarsening transform to the dataset\n",
    "# This will precompute pooling for 2 levels\n",
    "precoarsened_dataset = TUDataset(\n",
    "    root='/tmp/MUTAG_precoarsened',\n",
    "    name='MUTAG',\n",
    "    pre_transform=PreCoarsening(\n",
    "        pooler=ndp_pooler,\n",
    "        recursive_depth=2  # 2 levels of coarsening\n",
    "    ),\n",
    "    force_reload=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset with precoarsening:\")\n",
    "sample = precoarsened_dataset[0]\n",
    "print(sample)\n",
    "print(f\"\\nPooled data levels: {len(sample.pooled_data)}\")\n",
    "for i, pooled in enumerate(sample.pooled_data):\n",
    "    print(f\"  Level {i+1}: {pooled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pool-dataloader-header",
   "metadata": {},
   "source": [
    "We need a special [`PoolDataLoader`](https://torch-geometric-pool.readthedocs.io/en/latest/api/data/loaders.html#tgp.data.loaders.PoolDataLoader) to properly batch graphs with precoarsened data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pool-dataloader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PoolDataLoader instead of regular DataLoader\n",
    "precoarsened_train = precoarsened_dataset[:150]\n",
    "precoarsened_test = precoarsened_dataset[150:]\n",
    "\n",
    "pool_train_loader = PoolDataLoader(precoarsened_train, batch_size=32, shuffle=True)\n",
    "pool_test_loader = PoolDataLoader(precoarsened_test, batch_size=32)\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(pool_train_loader))\n",
    "print(f\"Batch: {batch}\")\n",
    "print(f\"\\nPooled data (properly batched):\")\n",
    "for i, pooled in enumerate(batch.pooled_data):\n",
    "    print(f\"  Level {i+1}: {pooled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precoarsening-model-header",
   "metadata": {},
   "source": [
    "Now we can build a model that uses the precomputed pooling structure. Notice that we only need `BaseReduce` - the $\\texttt{SEL}$ and $\\texttt{CON}$ operations are already precomputed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precoarsening-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecoarsenedGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_features = precoarsened_dataset.num_features\n",
    "        num_classes = precoarsened_dataset.num_classes\n",
    "        \n",
    "        # First MP layer\n",
    "        self.conv1 = ARMAConv(num_features, hidden_channels, num_layers=2)\n",
    "        \n",
    "        # Reducer (we only need REDUCE, SEL and CON are precomputed!)\n",
    "        self.reducer = BaseReduce()\n",
    "        \n",
    "        # MP layers after each pooling level\n",
    "        self.conv2 = ARMAConv(hidden_channels, hidden_channels, num_layers=2)\n",
    "        self.conv3 = ARMAConv(hidden_channels, hidden_channels, num_layers=2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # First MP layer on original graph\n",
    "        x = self.conv1(data.x, data.edge_index, data.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Apply precoarsened pooling levels\n",
    "        # Level 1\n",
    "        pooled_1 = data.pooled_data[0]\n",
    "        x, _ = self.reducer(x=x, so=pooled_1.so)  # Just REDUCE!\n",
    "        x = self.conv2(x, pooled_1.edge_index, pooled_1.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Level 2\n",
    "        pooled_2 = data.pooled_data[1]\n",
    "        x, _ = self.reducer(x=x, so=pooled_2.so)  # Just REDUCE!\n",
    "        x = self.conv3(x, pooled_2.edge_index, pooled_2.edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_reduce(x, reduce_op='sum', batch=pooled_2.batch)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "precoarsened_model = PrecoarsenedGNN(hidden_channels=64)\n",
    "print(precoarsened_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-precoarsened-header",
   "metadata": {},
   "source": [
    "Training is much faster since we skip $\\texttt{SEL}$ and $\\texttt{CON}$ operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-precoarsened",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = precoarsened_model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_precoarsened():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in pool_train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(pool_train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_precoarsened(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "print(\"Training with precoarsened pooling...\")\n",
    "for epoch in range(1, 21):\n",
    "    loss = train_precoarsened()\n",
    "    train_acc = test_precoarsened(pool_train_loader)\n",
    "    test_acc = test_precoarsened(pool_test_loader)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'\\nFinal Test Accuracy (Precoarsened): {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-pooler-header",
   "metadata": {},
   "source": [
    "## Creating Custom Poolers\n",
    "\n",
    "One of the most powerful features of tgp is the ability to create **custom pooling operators** by mixing and matching different $\\texttt{SEL}$, $\\texttt{RED}$, $\\texttt{CON}$, and $\\texttt{LIFT}$ components!\n",
    "\n",
    "Available components:\n",
    "- **Select**: [`TopkSelect`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.TopkSelect), [`MLPSelect`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.MLPSelect), [`NDPSelect`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.NDPSelect), etc.\n",
    "- **Connect**: [`SparseConnect`](https://torch-geometric-pool.readthedocs.io/en/latest/api/connect.html#tgp.connect.SparseConnect), [`DenseConnect`](https://torch-geometric-pool.readthedocs.io/en/latest/api/connect.html#tgp.connect.DenseConnect), [`KronConnect`](https://torch-geometric-pool.readthedocs.io/en/latest/api/connect.html#tgp.connect.KronConnect), etc.\n",
    "- **Reduce**: [`BaseReduce`](https://torch-geometric-pool.readthedocs.io/en/latest/api/reduce.html#tgp.reduce.BaseReduce)\n",
    "- **Lift**: [`BaseLift`](https://torch-geometric-pool.readthedocs.io/en/latest/api/lift.html#tgp.lift.BaseLift)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the following in the code cells below:\n",
    "\n",
    "1. **Part 1 – Custom pooler**: Implement a custom pooler that combines **TopkSelect** with **KronConnect** (instead of the default SparseConnect). Subclass `SRCPooling`; in `__init__` pass a `TopkSelect`, `BaseReduce`, `BaseLift`, and `KronConnect`. Implement `forward` to call select → reduce → connect and return a `PoolingOutput`. Create an instance and optionally compare with `get_pooler('topk', ...)`.\n",
    "\n",
    "2. **Part 2 – GNN**: Define a GNN that uses your custom pooler between two GCN layers (same structure as earlier in the tutorial), then global pool and classify. Use the existing `dataset`, `train_loader`, `test_loader`, and `train()` / `test()` from the notebook.\n",
    "\n",
    "3. **Part 3 – Train**: Train the model for 100 epochs and report final test accuracy.\n",
    "\n",
    "See the [Advanced](https://github.com/tgp-team/torch-geometric-pool/blob/main/docs/source/tutorials/advanced.ipynb) notebook for the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-pooler-exercise-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tgp.select import TopkSelect\n",
    "from tgp.connect import KronConnect\n",
    "from tgp.reduce import BaseReduce\n",
    "from tgp.lift import BaseLift\n",
    "from tgp.src import SRCPooling, PoolingOutput\n",
    "\n",
    "\n",
    "class CustomTopKKronPooler(SRCPooling):\n",
    "    # TODO: In __init__, call super().__init__ with selector=TopkSelect(...), reducer=BaseReduce(...), lifter=BaseLift(...), connector=KronConnect()\n",
    "    def __init__(self, in_channels, ratio=0.5):\n",
    "        pass  # TODO\n",
    "\n",
    "    # TODO: In forward, call self.select -> self.reduce -> self.connect, then return PoolingOutput(...)\n",
    "    def forward(self, x, adj, edge_weight=None, batch=None, **kwargs):\n",
    "        pass  # TODO\n",
    "\n",
    "\n",
    "# TODO: Create an instance, e.g. custom_pooler = CustomTopKKronPooler(in_channels=64, ratio=0.5)\n",
    "# TODO: Optionally compare with get_pooler('topk', in_channels=64, ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-pooler-usage-header",
   "metadata": {},
   "source": [
    "### Part 2: Use your pooler in a GNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-pooler-exercise-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithCustomPooler(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        # TODO: self.conv1 = GCNConv(...), self.pooler = CustomTopKKronPooler(...), self.conv2 = GCNConv(...), self.lin = Linear(...)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, batch):\n",
    "        # TODO: conv1 -> ReLU -> pooler -> conv2 -> ReLU -> global_pool -> dropout -> lin\n",
    "        pass  # TODO\n",
    "\n",
    "\n",
    "# TODO: custom_model = GNNWithCustomPooler(hidden_channels=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-custom-header",
   "metadata": {},
   "source": [
    "### Part 3: Train and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-custom-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: model = custom_model.to(device)\n",
    "# TODO: optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# TODO: Training loop for 100 epochs using train() and test(train_loader) / test(test_loader)\n",
    "# TODO: Print final test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we explored [tgp](https://github.com/tgp-team/torch-geometric-pool) for building hierarchical Graph Neural Networks with pooling.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **SRC Framework**: All pooling operators decompose into $\\texttt{SEL}$, $\\texttt{RED}$, and $\\texttt{CON}$ operations (plus $\\texttt{LIFT}$ for unpooling)\n",
    "\n",
    "2. **Easy Integration**: Adding pooling to a GNN is as simple as inserting a pooler between message-passing layers\n",
    "\n",
    "3. **Sparse vs Dense Poolers**:\n",
    "   - Sparse poolers work with edge lists (TopK, SAG, ASAP)\n",
    "   - Dense poolers work with dense adjacency matrices (MinCut, DiffPool, DMoN)\n",
    "   - Dense poolers often have auxiliary losses\n",
    "\n",
    "4. **Hierarchical Architectures**: Stack multiple pooling layers to learn multi-scale representations\n",
    "\n",
    "5. **Performance Optimization**:\n",
    "   - **Node classification**: Encoder–pool–bottleneck–lift–decoder; use \\(x, adj, batch\\) for the pooler.\n",
    "   - **Precoarsening**: For non-learnable poolers, precompute pooling structures\n",
    "\n",
    "6. **Modularity**: Create custom poolers by combining different $\\texttt{SEL}$, $\\texttt{RED}$, $\\texttt{CON}$, and $\\texttt{LIFT}$ components\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Documentation](https://torch-geometric-pool.readthedocs.io)\n",
    "- [GitHub Repository](https://github.com/tgp-team/torch-geometric-pool)\n",
    "- [tgp Paper](https://arxiv.org/abs/2512.12642)\n",
    "- [SRC Framework Paper](https://arxiv.org/abs/2110.05292)\n",
    "\n",
    "Try different poolers on your own datasets and see which works best for your task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
