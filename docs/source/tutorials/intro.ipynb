{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb007bcb8844ab8",
   "metadata": {},
   "source": [
    "# Introduction by examples\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tgp-team/torch-geometric-pool/blob/main/docs/source/tutorials/intro.ipynb)\n",
    "\n",
    "In the following, we will go through a few examples that showcase the main functionalities of <img src=\"../_static/img/tgp-logo.svg\" width=\"40px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> TGP.\n",
    "Let's start by importing the required libraries and checking the pooling operators that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688adfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "if 'google.colab' in sys.modules:\n",
    "    import os\n",
    "    os.environ['TORCH'] = torch.__version__\n",
    "    print(torch.__version__)\n",
    "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "    !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q torch-geometric-pool[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109f9184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available poolers:\n",
      "  ASAPooling\n",
      "  AsymCheegerCutPooling\n",
      "  DiffPool\n",
      "  DMoNPooling\n",
      "  EdgeContractionPooling\n",
      "  GraclusPooling\n",
      "  HOSCPooling\n",
      "  LaPooling\n",
      "  JustBalancePooling\n",
      "  KMISPooling\n",
      "  MinCutPooling\n",
      "  NDPPooling\n",
      "  NMFPooling\n",
      "  PANPooling\n",
      "  SAGPooling\n",
      "  TopkPooling\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, GCNConv\n",
    "\n",
    "from tgp.poolers import TopkPooling, get_pooler, pooler_classes, pooler_map\n",
    "\n",
    "torch.set_printoptions(threshold=2, edgeitems=2)\n",
    "\n",
    "print(\"Available poolers:\")\n",
    "for pooler in pooler_classes:\n",
    "    print(f\"  {pooler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f001ce1",
   "metadata": {},
   "source": [
    "For example, let's create a [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9496bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooler: TopkPooling(\n",
      "\tselect=TopkSelect(in_channels=16, ratio=0.5, act=Tanh(), s_inv_op=transpose)\n",
      "\treduce=BaseReduce(reduce_op=sum)\n",
      "\tlift=BaseLift(matrix_op=precomputed, reduce_op=sum)\n",
      "\tconnect=SparseConnect(reduce_op=sum, remove_self_loops=True)\n",
      "\tmultiplier=1.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pooler = TopkPooling(in_channels=16)\n",
    "print(f\"Pooler: {pooler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941da43f",
   "metadata": {},
   "source": [
    "Each pooler is associated with an alias that can be used to quickly instantiate a pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3aed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available poolers:\n",
      "'asap' --> ASAPooling\n",
      "'acc' --> AsymCheegerCutPooling\n",
      "'diff' --> DiffPool\n",
      "'dmon' --> DMoNPooling\n",
      "'ec' --> EdgeContractionPooling\n",
      "'graclus' --> GraclusPooling\n",
      "'hosc' --> HOSCPooling\n",
      "'lap' --> LaPooling\n",
      "'jb' --> JustBalancePooling\n",
      "'kmis' --> KMISPooling\n",
      "'mincut' --> MinCutPooling\n",
      "'ndp' --> NDPPooling\n",
      "'nmf' --> NMFPooling\n",
      "'pan' --> PANPooling\n",
      "'sag' --> SAGPooling\n",
      "'topk' --> TopkPooling\n"
     ]
    }
   ],
   "source": [
    "print(\"Available poolers:\")\n",
    "for alias, cls in zip(pooler_map.keys(), pooler_map.values()):\n",
    "    print(f\"'{alias}' --> {cls.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659ed4e",
   "metadata": {},
   "source": [
    "We can instantiate the same object of class [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) by passing the alias and a dict with the parameters needed to initialize the pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32397974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopkPooling(\n",
      "\tselect=TopkSelect(in_channels=3, ratio=0.25, act=Tanh(), s_inv_op=transpose)\n",
      "\treduce=BaseReduce(reduce_op=sum)\n",
      "\tlift=BaseLift(matrix_op=precomputed, reduce_op=sum)\n",
      "\tconnect=SparseConnect(reduce_op=sum, remove_self_loops=True)\n",
      "\tmultiplier=1.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"in_channels\": 3,  # Number of input features\n",
    "    \"ratio\": 0.25,  # Percentage of nodes to keep\n",
    "}\n",
    "\n",
    "pooler = get_pooler(\"topk\", **params)  # Get the pooler by alias\n",
    "print(pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf6ba2",
   "metadata": {},
   "source": [
    "We see that each pooling layer implements a specific select ($\\texttt{SEL}$), reduce ($\\texttt{RED}$), connect $\\texttt{CON}$ operations, as defined by the [SRC framework](https://arxiv.org/abs/2110.05292).\n",
    "\n",
    "<img src=\"../_static/img/src_overview.png\" style=\"width: 55%; display: block; margin: auto;\">\n",
    "\n",
    "- The $\\texttt{SEL}$ operation is what sets most pooling methods apart and defines how the nodes are assigned to the supernodes of the pooled graph. \n",
    "- The $\\texttt{RED}$ operation specifies how to compute the features of the supernodes in the pooled graph. \n",
    "- Finally, $\\texttt{CON}$ is creates the connectivity matrix of the pooled graph. \n",
    "\n",
    "The pooling operators also have a $\\texttt{LIFT}$ function, which is used by some GNN architectures to map the pooled node features back to the space of the original graph.\n",
    "See [here](../content/src.md) for an introduction to the SRC(L) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63799fa6",
   "metadata": {},
   "source": [
    "## Calling a pooling layer\n",
    "\n",
    "A pooling layer can be called similarly to a message-passing layer in PyG.\n",
    "Let's start by loading some data and creating a data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e961a244cd6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ENZYMES(600)\n",
      "Data batch: DataBatch(edge_index=[2, 4242], x=[1127, 3], y=[32], batch=[1127], ptr=[33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root=\"/tmp/ENZYMES\", name=\"ENZYMES\")\n",
    "print(f\"Dataset: {dataset}\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "data_batch = next(iter(loader))\n",
    "print(f\"Data batch: {data_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536333e",
   "metadata": {},
   "source": [
    "```{attention}\n",
    "Pooling operators support **edge weights**, i.e., scalar values stored in a `edge_weight` attribute.\n",
    "However, some dataset have **edge features** stored in the `edge_attr` field.\n",
    "In <img src=\"../_static/img/tgp-logo.svg\" width=\"40px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp we assume that the edge attributes are processed by the message-passing layers before pooling, which embed the attributes into the node features that reach the pooling operators.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd64f78221fc8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolingOutput(so=[1127, 291], x=[291, 3], edge_index=[2, 576], edge_weight=None, batch=[291], loss=None)\n"
     ]
    }
   ],
   "source": [
    "pooling_output = pooler(\n",
    "    x=data_batch.x,\n",
    "    adj=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(pooling_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725886c",
   "metadata": {},
   "source": [
    "The output of a pooling layer is an object of class [`PoolingOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/src.html#tgp.src.PoolingOutput) that contains different fields:\n",
    "- the node features of the pooled graph (`x`), \n",
    "- the indices and weights of the pooled adjacency matrix (`edge_index`, `edge_weight`), \n",
    "- the batch indices of the pooled graphs (`batch`). \n",
    "\n",
    "In addition, `so` is an object of class [`SelectOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.SelectOutput), i.e., the output of the $\\texttt{SEL}$ operation that describes how the nodes of the original graph are assigned to the supernodes of the pooled graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c8b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOutput(num_nodes=1127, num_clusters=291)\n"
     ]
    }
   ],
   "source": [
    "print(pooling_output.so)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8c3af",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Some pooling operators save additional data structures in the [`PoolingOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/src.html#tgp.src.PoolingOutput), to be used downstream the $\\texttt{RES}$ and $\\texttt{CON}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3654c",
   "metadata": {},
   "source": [
    "The pooling layer can also be used to perform $\\texttt{LIFT}$, i.e., to map the pooled features back to the node original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cbe16db7b320633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x shape: torch.Size([1127, 3])\n",
      "pooled x shape: torch.Size([291, 3])\n",
      "x_lift shape: torch.Size([1127, 3])\n"
     ]
    }
   ],
   "source": [
    "x_lift = pooler(\n",
    "    x=pooling_output.x, so=pooling_output.so, batch=pooling_output.batch, lifting=True\n",
    ")\n",
    "\n",
    "print(f\"original x shape: {data_batch.x.shape}\")\n",
    "print(f\"pooled x shape: {pooling_output.x.shape}\")\n",
    "print(f\"x_lift shape: {x_lift.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27ee46",
   "metadata": {},
   "source": [
    "$\\texttt{LIFT}$ is typically used by GNNs with an autoencoder architecture that perform node-level tasks (e.g., node classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f80d4",
   "metadata": {},
   "source": [
    "## Types of pooling operator\n",
    "\n",
    "On of the main differnces between the pooling operators in <img src=\"../_static/img/tgp-logo.svg\" width=\"40px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp is if they are **dense** or **sparse**. [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) that we just saw is a sparse method. Let's now look at a dense pooling methods: [`MinCutPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.MinCutPooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b83aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinCutPooling(\n",
      "\tselect=DenseSelect(in_channels=[3], k=10, act=None, dropout=0.0, s_inv_op=transpose)\n",
      "\treduce=BaseReduce(reduce_op=sum)\n",
      "\tlift=BaseLift(matrix_op=precomputed, reduce_op=sum)\n",
      "\tconnect=DenseConnect(remove_self_loops=False, degree_norm=False, adj_transpose=True)\n",
      "\tcut_loss_coeff=1.0\n",
      "\tortho_loss_coeff=1.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"in_channels\": 3,  # Number of input features\n",
    "    \"k\": 10,  # Number of supernodes in the pooled graph\n",
    "}\n",
    "\n",
    "dense_pooler = get_pooler(\"mincut\", **params)\n",
    "print(dense_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e99439",
   "metadata": {},
   "source": [
    "Something that sets sparse and pooling methods apart is the format of the data that they take as input. \n",
    "In particular, dense methods take as input graphs whose connectivity matrix is a dense tensor.\n",
    "\n",
    "Each pooling operator in <img src=\"../_static/img/tgp-logo.svg\" width=\"40px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp provides a preprocessing function that converts the data in the correct format accepted by the operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7ddfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dense shape: torch.Size([32, 57, 3])\n",
      "adj_dense shape: torch.Size([32, 57, 57])\n",
      "mask shape: torch.Size([32, 57])\n"
     ]
    }
   ],
   "source": [
    "x_dense, adj_dense, mask = dense_pooler.preprocessing(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(f\"x_dense shape: {x_dense.shape}\")\n",
    "print(f\"adj_dense shape: {adj_dense.shape}\")\n",
    "print(f\"mask shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b691071",
   "metadata": {},
   "source": [
    "The processed data can now be fed into the dense pooling operator to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0280d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolingOutput(so=[57, 10], x=[32, 10, 3], edge_index=[32, 10, 10], edge_weight=None, batch=None, loss=['cut_loss', 'ortho_loss'])\n"
     ]
    }
   ],
   "source": [
    "dense_pooling_output = dense_pooler(\n",
    "    x=x_dense,\n",
    "    adj=adj_dense,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(dense_pooling_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7292e",
   "metadata": {},
   "source": [
    "The connectivity of the coarsened graphs generated by a dense pooling operator is also a dense tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f354059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1001,  ..., 0.1359, 0.1238],\n",
      "        [0.1001, 0.0000,  ..., 0.1278, 0.1171],\n",
      "        ...,\n",
      "        [0.1359, 0.1278,  ..., 0.0000, 0.1576],\n",
      "        [0.1238, 0.1171,  ..., 0.1576, 0.0000]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(dense_pooling_output.edge_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c6c3e",
   "metadata": {},
   "source": [
    "Another difference w.r.t. to what we saw with [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) is the presence of one or more loss terms in the pooling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d31bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut_loss: -0.974\n",
      "ortho_loss: 1.160\n"
     ]
    }
   ],
   "source": [
    "for key, value in dense_pooling_output.loss.items():\n",
    "    print(f\"{key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24afe5",
   "metadata": {},
   "source": [
    "These are *auxiliary losses* that must be minimized along with the other task's losses used to train the GNN. \n",
    "Most dense pooling methods have an auxiliary loss. \n",
    "A few sparse methods have an auxiliary loss too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ba904",
   "metadata": {},
   "source": [
    "## GNN model with pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14240f",
   "metadata": {},
   "source": [
    "Let's create a simple GNN for graph classification with the following architecture: \n",
    "\n",
    "$$[\\texttt{MP}-\\texttt{Pool}-\\texttt{MP}-\\texttt{GlobalPool}-\\texttt{Linear}]$$\n",
    "\n",
    "\n",
    "### Initialization\n",
    "First, in the `__init__()` we specify the architecture, instatiating the pooling layer from its alias and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "412fb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, pooler_type, pooler_kwargs, hidden_channels=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # First MP layer\n",
    "        self.conv1 = GCNConv(in_channels=in_channels, out_channels=hidden_channels)\n",
    "\n",
    "        # Pooling\n",
    "        self.pooler = pooler_kwargs.update({\"in_channels\": hidden_channels})\n",
    "        self.pooler = get_pooler(pooler_type, **pooler_kwargs)\n",
    "\n",
    "        # Second MP layer\n",
    "        if self.pooler.is_dense:\n",
    "            self.conv2 = DenseGCNConv(\n",
    "                in_channels=hidden_channels, out_channels=hidden_channels\n",
    "            )\n",
    "        else:\n",
    "            self.conv2 = GCNConv(\n",
    "                in_channels=hidden_channels, out_channels=hidden_channels\n",
    "            )\n",
    "\n",
    "        # Readout layer\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415807d",
   "metadata": {},
   "source": [
    "Note that the type of pooling operator determines what kind of MP layer is used after pooling. \n",
    "A sparse pooler is followed by a sparse MP operator such as [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/2.6.1/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv). \n",
    "On the other hand, a dense pooling operator that returns a dense connectivity matrix must be followed by a dense MP layer such as [`DenseGCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.DenseGCNConv.html).\n",
    "\n",
    "The type of pooling operator can be checked by the property `is_dense`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee3d34",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Next, we define the forward pass of the GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c28945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, edge_index, edge_weight, batch=None):\n",
    "    # First MP layer\n",
    "    x = self.conv1(x, edge_index, edge_weight)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # Pooling\n",
    "    x, edge_index, mask = self.pooler.preprocessing(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_weight=edge_weight,\n",
    "        batch=batch,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    out = self.pooler(\n",
    "        x=x, adj=edge_index, edge_weight=edge_weight, batch=batch, mask=mask\n",
    "    )\n",
    "    x_pool, adj_pool = out.x, out.edge_index\n",
    "\n",
    "    # Second MP layer\n",
    "    x = self.conv2(x_pool, adj_pool)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # Global pooling\n",
    "    x = self.pooler.global_pool(x, reduce_op=\"sum\", batch=out.batch)\n",
    "\n",
    "    # Readout layer\n",
    "    x = self.lin(x)\n",
    "\n",
    "    if out.loss is not None:\n",
    "        return F.log_softmax(x, dim=-1), sum(out.get_loss_value())\n",
    "    else:\n",
    "        return F.log_softmax(x, dim=-1), torch.tensor(0.0)\n",
    "\n",
    "\n",
    "GNN.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3358d75",
   "metadata": {},
   "source": [
    "There are a few things to discuss.\n",
    "\n",
    "#### Preprocessing\n",
    "In the `forward()` function, before calling the pooling layer, we must preprocess the data. If the pooler is sparse, preprocessing has no effect: `x` and `edge_index` will be returned as-is, and `mask` will be `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6165d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1127, 3])\n",
      "edge_index shape: torch.Size([2, 4242])\n",
      "mask: None\n"
     ]
    }
   ],
   "source": [
    "x, edge_index, mask = pooler.preprocessing(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"edge_index shape: {edge_index.shape}\")\n",
    "print(f\"mask: {mask}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6cd9d",
   "metadata": {},
   "source": [
    "Conversely, if the pooler is dense, `x` will be a tensor of size $[B, N, F]$, where $B$ is the batch size, $N$ is the maximum number of nodes in the batch, and $F$ is the size of the node features. \n",
    "Nodes with less than $N$ nodes will be padded and `mask` is a boolean indicating which node is valid or not. \n",
    "Similarly, `edge_index` will be a dense tensor of shape $[B, N, N]$. \n",
    "Internally, `preprocessing()` of a dense pooler calls the functions [`to_dense_batch`](https://pytorch-geometric.readthedocs.io/en/2.4.0/_modules/torch_geometric/utils/to_dense_batch.html) and [`to_dense_adj`](https://pytorch-geometric.readthedocs.io/en/2.4.0/_modules/torch_geometric/utils/to_dense_adj.html) of <img src=\"https://raw.githubusercontent.com/TorchSpatiotemporal/tsl/main/docs/source/_static/img/logos/pyg.svg\" width=\"20px\" align=\"center\"/> PyG.\n",
    "\n",
    "Finally, `use_cache=True`, avoids recomputing the densified version of `edge_index`. \n",
    "This is useful in tasks such as [node_classification](https://github.com/tgp-team/torch-geometric-pool/blob/main/examples/node_class.py) and [clustering](https://github.com/tgp-team/torch-geometric-pool/blob/main/examples/clustering.py), where there is only one  underlying graph that is usually large and costly to densify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14595130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([32, 57, 3])\n",
      "edge_index shape: torch.Size([32, 57, 57])\n",
      "mask shape: torch.Size([32, 57])\n"
     ]
    }
   ],
   "source": [
    "x, edge_index, mask = dense_pooler.preprocessing(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"edge_index shape: {edge_index.shape}\")\n",
    "print(f\"mask shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6cb40",
   "metadata": {},
   "source": [
    "#### Global pooling\n",
    "The global pooling operation combines all the features in the current graphs and is implemented differently depending if the pooler is sparse or dense.\n",
    "\n",
    "In the sparse case, we have a `batch` tensor indicating to which graph each node belongs to. \n",
    "In this case, global pooling should combine the features of the nodes beloning to the same graph. The output is a tensor of shape $[B, F]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "055cff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1127, 3])\n",
      "Output shape: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Sparse case\n",
    "print(f\"Input shape: {data_batch.x.shape}\")\n",
    "out_global_sparse = pooler.global_pool(\n",
    "    data_batch.x, reduce_op=\"sum\", batch=data_batch.batch\n",
    ")\n",
    "print(f\"Output shape: {out_global_sparse.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf0a76",
   "metadata": {},
   "source": [
    "In the dense case, the features of the pooled graph are stored in a tensor of shape $[B, K, F]$ and global pooling can be done e.g., by summing or taking the average across the node dimension, yielding a tensor of shape $[B, F]$. In this case, `batch` is not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "723065eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 57, 3])\n",
      "Output shape: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Dense case\n",
    "print(f\"Input shape: {x_dense.shape}\")\n",
    "out_global_dense = dense_pooler.global_pool(x_dense, reduce_op=\"sum\", batch=None)\n",
    "print(f\"Output shape: {out_global_dense.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a8cdf",
   "metadata": {},
   "source": [
    "Note that in both cases the output is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3057336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(out_global_sparse, out_global_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e370e6",
   "metadata": {},
   "source": [
    "#### Auxiliary losses\n",
    "Finally, as we saw earlier some pooling operator return an auxiliary loss, while others do not.\n",
    "In the forward pass we can check if `out.loss` is not `None` and, in case, return the sum of all the auxiliary losses to be passed to the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ad67f",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Let's first test our GNN when configured with a sparse pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "103ee50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse GNN output shape: torch.Size([32, 6])\n",
      "Sparse GNN loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "sparse_params = {\n",
    "    \"ratio\": 0.25,  # Percentage of nodes to keep\n",
    "}\n",
    "\n",
    "sparse_pool_gnn = GNN(\n",
    "    in_channels=num_features,\n",
    "    out_channels=num_classes,\n",
    "    pooler_type=\"topk\",\n",
    "    pooler_kwargs=sparse_params,\n",
    ")\n",
    "\n",
    "sparse_gnn_out = sparse_pool_gnn(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(f\"Sparse GNN output shape: {sparse_gnn_out[0].shape}\")\n",
    "print(f\"Sparse GNN loss: {sparse_gnn_out[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358d3da",
   "metadata": {},
   "source": [
    "Since there is no auxiliary loss, the second output of the GNN is simply a constant zero-valued tensor that will not affect the gradients computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d241a54",
   "metadata": {},
   "source": [
    "Next, we create the GNN with the dense pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8ac867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense GNN output shape: torch.Size([32, 6])\n",
      "Dense GNN loss: 0.169\n"
     ]
    }
   ],
   "source": [
    "dense_params = {\n",
    "    \"k\": 10,  # Number of supernodes in the pooled graph\n",
    "}\n",
    "dense_pool_gnn = GNN(\n",
    "    in_channels=num_features,\n",
    "    out_channels=num_classes,\n",
    "    pooler_type=\"mincut\",\n",
    "    pooler_kwargs=dense_params,\n",
    ")\n",
    "dense_gnn_out = dense_pool_gnn(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(f\"Dense GNN output shape: {dense_gnn_out[0].shape}\")\n",
    "print(f\"Dense GNN loss: {dense_gnn_out[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f379c1",
   "metadata": {},
   "source": [
    "This time, we get an auxiliary loss that should be added to the other losses, e.g. the classification loss or other losses of the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ed0c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.324\n"
     ]
    }
   ],
   "source": [
    "total_loss = F.nll_loss(dense_gnn_out[0], data_batch.y.view(-1)) + dense_gnn_out[1]\n",
    "print(f\"Loss: {total_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469ede0",
   "metadata": {},
   "source": [
    "And that's it! We can train this GNN as any other that we normally build with <img src=\"https://raw.githubusercontent.com/TorchSpatiotemporal/tsl/main/docs/source/_static/img/logos/pyg.svg\" width=\"20px\" align=\"center\"/> PyG.\n",
    "\n",
    "You can check the complete graph classificatione example [here](https://github.com/tgp-team/torch-geometric-pool/blob/main/examples/classification.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgp-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
