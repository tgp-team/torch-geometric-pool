{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb007bcb8844ab8",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tgp-team/torch-geometric-pool/blob/main/docs/source/tutorials/quick_start.ipynb)\n",
    "\n",
    "In the following, we will go through a few examples that showcase the main functionalities of <img src=\"../_static/img/tgp-logo.svg\" width=\"20px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp.\n",
    "\n",
    "If you're running this in Google Colab, you'll need to restart the session after running the installation cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688adfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    %pip install torch==2.4.1 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b022a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    %pip install torch_geometric==2.6.1\n",
    "    %pip install torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
    "    %pip install pygsp==0.6.1\n",
    "    %pip install -q git+https://github.com/tgp-team/torch-geometric-pool.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e36e79",
   "metadata": {},
   "source": [
    "Let's start by importing the required libraries and checking the pooling operators that are available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109f9184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available poolers:\n",
      "1. ASAPooling\n",
      "2. AsymCheegerCutPooling\n",
      "3. BNPool\n",
      "4. DiffPool\n",
      "5. DMoNPooling\n",
      "6. EdgeContractionPooling\n",
      "7. EigenPooling\n",
      "8. GraclusPooling\n",
      "9. HOSCPooling\n",
      "10. LaPooling\n",
      "11. JustBalancePooling\n",
      "12. KMISPooling\n",
      "13. MaxCutPooling\n",
      "14. MinCutPooling\n",
      "15. NDPPooling\n",
      "16. NMFPooling\n",
      "17. NoPool\n",
      "18. PANPooling\n",
      "19. SAGPooling\n",
      "20. TopkPooling\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, GCNConv\n",
    "\n",
    "from tgp.poolers import TopkPooling, get_pooler, pooler_classes, pooler_map\n",
    "\n",
    "torch.set_printoptions(threshold=2, edgeitems=2)\n",
    "\n",
    "print(\"Available poolers:\")\n",
    "for i,pooler in enumerate(pooler_classes):\n",
    "    print(f\"{i+1}. {pooler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f001ce1",
   "metadata": {},
   "source": [
    "For example, let's create a [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9496bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooler: TopkPooling(\n",
      "\tselect=TopkSelect(in_channels=16, ratio=0.5, act=Tanh(), s_inv_op=transpose)\n",
      "\treduce=BaseReduce(reduce_op=sum)\n",
      "\tlift=BaseLift(matrix_op=precomputed, reduce_op=sum)\n",
      "\tconnect=SparseConnect(reduce_op=sum, remove_self_loops=True, edge_weight_norm=False, degree_norm=False)\n",
      "\tmultiplier=1.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pooler = TopkPooling(in_channels=16)\n",
    "print(f\"Pooler: {pooler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941da43f",
   "metadata": {},
   "source": [
    "Each pooler is associated with an alias that can be used to quickly instantiate a pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3aed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available poolers:\n",
      "'asap' --> ASAPooling\n",
      "'acc' --> AsymCheegerCutPooling\n",
      "'bnpool' --> BNPool\n",
      "'diff' --> DiffPool\n",
      "'dmon' --> DMoNPooling\n",
      "'ec' --> EdgeContractionPooling\n",
      "'eigen' --> EigenPooling\n",
      "'graclus' --> GraclusPooling\n",
      "'hosc' --> HOSCPooling\n",
      "'lap' --> LaPooling\n",
      "'jb' --> JustBalancePooling\n",
      "'kmis' --> KMISPooling\n",
      "'maxcut' --> MaxCutPooling\n",
      "'mincut' --> MinCutPooling\n",
      "'ndp' --> NDPPooling\n",
      "'nmf' --> NMFPooling\n",
      "'nopool' --> NoPool\n",
      "'pan' --> PANPooling\n",
      "'sag' --> SAGPooling\n",
      "'topk' --> TopkPooling\n"
     ]
    }
   ],
   "source": [
    "print(\"Available poolers:\")\n",
    "for alias, cls in zip(pooler_map.keys(), pooler_map.values()):\n",
    "    print(f\"'{alias}' --> {cls.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659ed4e",
   "metadata": {},
   "source": [
    "We can instantiate the same object of class [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) by passing the alias and a dict with the parameters needed to initialize the pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32397974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopkPooling(\n",
      "\tselect=TopkSelect(in_channels=3, ratio=0.25, act=Tanh(), s_inv_op=transpose)\n",
      "\treduce=BaseReduce(reduce_op=sum)\n",
      "\tlift=BaseLift(matrix_op=precomputed, reduce_op=sum)\n",
      "\tconnect=SparseConnect(reduce_op=sum, remove_self_loops=True, edge_weight_norm=False, degree_norm=False)\n",
      "\tmultiplier=1.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"in_channels\": 3,  # Number of input features\n",
    "    \"ratio\": 0.25,  # Percentage of nodes to keep\n",
    "}\n",
    "\n",
    "pooler = get_pooler(\"topk\", **params)  # Get the pooler by alias\n",
    "print(pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf6ba2",
   "metadata": {},
   "source": [
    "We see that each pooling layer implements a specific select ($\\texttt{SEL}$), reduce ($\\texttt{RED}$), connect $\\texttt{CON}$ operations, as defined by the [SRC framework](https://arxiv.org/abs/2110.05292).\n",
    "\n",
    "<img src=\"../_static/img/src_overview.png\" style=\"width: 55%; display: block; margin: auto;\">\n",
    "\n",
    "- The $\\texttt{SEL}$ operation is what sets most pooling methods apart and defines how the nodes are assigned to the supernodes of the pooled graph. \n",
    "- The $\\texttt{RED}$ operation specifies how to compute the features of the supernodes in the pooled graph. \n",
    "- Finally, $\\texttt{CON}$ creates the connectivity matrix of the pooled graph. \n",
    "\n",
    "The pooling operators also have a $\\texttt{LIFT}$ function, which is used by some GNN architectures to map the pooled node features back to the node space of the original graph.\n",
    "See [here](../content/src.md) for an introduction to the SRC(L) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63799fa6",
   "metadata": {},
   "source": [
    "## Calling a pooling layer\n",
    "\n",
    "A pooling layer can be called similarly to a message-passing layer in PyG.\n",
    "Let's start by loading some data and creating a data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e961a244cd6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ENZYMES(600)\n",
      "Data batch: DataBatch(edge_index=[2, 4078], x=[1051, 3], y=[32], batch=[1051], ptr=[33])\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root=\"/tmp/ENZYMES\", name=\"ENZYMES\")\n",
    "print(f\"Dataset: {dataset}\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "data_batch = next(iter(loader))\n",
    "print(f\"Data batch: {data_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536333e",
   "metadata": {},
   "source": [
    "Pooling operators support **edge weights**, i.e., scalar values stored in a `edge_weight` attribute.\n",
    "However, some dataset have **edge features** stored in the `edge_attr` field.\n",
    "In <img src=\"../_static/img/tgp-logo.svg\" width=\"40px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp we assume that the edge attributes are processed by the message-passing layers before pooling, which embed the attributes into the node features that reach the pooling operators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd64f78221fc8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolingOutput(so=[1051, 272], x=[272, 3], edge_index=[2, 422], edge_weight=None, batch=[272], loss=None)\n"
     ]
    }
   ],
   "source": [
    "pooling_output = pooler(\n",
    "    x=data_batch.x,\n",
    "    adj=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(pooling_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725886c",
   "metadata": {},
   "source": [
    "The output of a pooling layer is an object of class [`PoolingOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/src.html#tgp.src.PoolingOutput) that contains different fields:\n",
    "- the node features of the pooled graph (`x`), \n",
    "- the indices and weights of the pooled adjacency matrix (`edge_index`, `edge_weight`), \n",
    "- the batch indices of the pooled graphs (`batch`). \n",
    "\n",
    "In addition, `so` is an object of class [`SelectOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/select.html#tgp.select.SelectOutput), i.e., the output of the $\\texttt{SEL}$ operation that describes how the nodes of the original graph are assigned to the supernodes of the pooled graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c8b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectOutput(num_nodes=1051, num_supernodes=272)\n"
     ]
    }
   ],
   "source": [
    "print(pooling_output.so)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8c3af",
   "metadata": {},
   "source": [
    "Some pooling operators save additional data structures in the [`PoolingOutput`](https://torch-geometric-pool.readthedocs.io/en/latest/api/src.html#tgp.src.PoolingOutput), to be used downstream the $\\texttt{RES}$ and $\\texttt{CON}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3654c",
   "metadata": {},
   "source": [
    "The pooling layer can also be used to perform $\\texttt{LIFT}$, i.e., to map the pooled features back to the original node space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cbe16db7b320633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x shape: torch.Size([1051, 3])\n",
      "pooled x shape: torch.Size([272, 3])\n",
      "x_lift shape: torch.Size([1051, 3])\n"
     ]
    }
   ],
   "source": [
    "x_lift = pooler(\n",
    "    x=pooling_output.x, so=pooling_output.so, batch=pooling_output.batch, lifting=True\n",
    ")\n",
    "\n",
    "print(f\"original x shape: {data_batch.x.shape}\")\n",
    "print(f\"pooled x shape: {pooling_output.x.shape}\")\n",
    "print(f\"x_lift shape: {x_lift.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27ee46",
   "metadata": {},
   "source": [
    "$\\texttt{LIFT}$ is typically used by GNNs with an autoencoder architecture that perform node-level tasks (e.g., node classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f80d4",
   "metadata": {},
   "source": [
    "## Types of pooling operator\n",
    "\n",
    "On of the main differnces between the pooling operators in <img src=\"../_static/img/tgp-logo.svg\" width=\"20px\" align=\"center\" style=\"display: inline-block; height: 1.3em; width: unset; vertical-align: text-top;\"/> tgp is if they are **dense** or **sparse**. [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) that we just saw is a sparse method. Let's now look at a dense pooler: [`MinCutPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.MinCutPooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b83aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinCutPooling(\n",
      "\tselect=MLPSelect(in_channels=[3], k=10, act=None, dropout=0.0, s_inv_op=transpose)\n",
      "\treduce=BaseReduce(reduce_op=sum)\n",
      "\tlift=BaseLift(matrix_op=precomputed, reduce_op=sum)\n",
      "\tconnect=DenseConnect(remove_self_loops=True, degree_norm=True, adj_transpose=True, edge_weight_norm=False, sparse_output=False)\n",
      "\tbatched=True\n",
      "\tcut_loss_coeff=1.0\n",
      "\tortho_loss_coeff=1.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"in_channels\": 3,  # Number of input features\n",
    "    \"k\": 10,  # Number of supernodes in the pooled graph\n",
    "}\n",
    "\n",
    "dense_pooler = get_pooler(\"mincut\", **params)\n",
    "print(dense_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e99439",
   "metadata": {},
   "source": [
    "Something that sets sparse and dense pooling methods apart is the format of the data they use internally. \n",
    "In tgp, all poolers are called with the same interface: pass node features, edge index, and batch vector. \n",
    "Poolers accept \\((x, adj, batch)\\) directly; dense poolers handle conversion to dense format internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7ddfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolingOutput(so=[88, 10], x=[32, 10, 3], edge_index=[32, 10, 10], edge_weight=None, batch=None, loss=['cut_loss', 'ortho_loss'])\n"
     ]
    }
   ],
   "source": [
    "dense_pooling_output = dense_pooler(\n",
    "    x=data_batch.x,\n",
    "    adj=data_batch.edge_index,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(dense_pooling_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7292e",
   "metadata": {},
   "source": [
    "The connectivity of the coarsened graphs generated by a dense pooling operator is also a dense tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f354059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1271,  ..., 0.1155, 0.1050],\n",
      "        [0.1271, 0.0000,  ..., 0.1038, 0.0929],\n",
      "        ...,\n",
      "        [0.1155, 0.1038,  ..., 0.0000, 0.0848],\n",
      "        [0.1050, 0.0929,  ..., 0.0848, 0.0000]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(dense_pooling_output.edge_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c6c3e",
   "metadata": {},
   "source": [
    "Another difference w.r.t. [`TopkPooling`](https://torch-geometric-pool.readthedocs.io/en/latest/api/poolers.html#tgp.poolers.TopkPooling) is the presence of one or more loss terms in the pooling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d31bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut_loss: -0.963\n",
      "ortho_loss: 1.155\n"
     ]
    }
   ],
   "source": [
    "for key, value in dense_pooling_output.loss.items():\n",
    "    print(f\"{key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24afe5",
   "metadata": {},
   "source": [
    "These are *auxiliary losses* that must be minimized along with the other task's losses used to train the GNN. \n",
    "Most dense pooling methods have an auxiliary loss. \n",
    "A few sparse methods have an auxiliary loss too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ba904",
   "metadata": {},
   "source": [
    "## GNN model with pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14240f",
   "metadata": {},
   "source": [
    "Let's create a simple GNN for graph classification with the following architecture: \n",
    "\n",
    "$$[\\texttt{MP}-\\texttt{Pool}-\\texttt{MP}-\\texttt{GlobalPool}-\\texttt{Linear}]$$\n",
    "\n",
    "\n",
    "### Initialization\n",
    "First, in the `__init__()` we specify the architecture, instatiating the MP layers, the pooling layer from its alias and parameters, and the readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412fb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, pooler_type, pooler_kwargs, hidden_channels=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # First MP layer\n",
    "        self.conv1 = GCNConv(in_channels=in_channels, out_channels=hidden_channels)\n",
    "\n",
    "        # Pooling\n",
    "        self.pooler = pooler_kwargs.update({\"in_channels\": hidden_channels})\n",
    "        self.pooler = get_pooler(pooler_type, **pooler_kwargs)\n",
    "\n",
    "        # Second MP layer\n",
    "        if self.pooler.is_dense:\n",
    "            self.conv2 = DenseGCNConv(\n",
    "                in_channels=hidden_channels, out_channels=hidden_channels\n",
    "            )\n",
    "        else:\n",
    "            self.conv2 = GCNConv(\n",
    "                in_channels=hidden_channels, out_channels=hidden_channels\n",
    "            )\n",
    "\n",
    "        # Readout layer\n",
    "        self.lin = torch.nn.Linear(hidden_channels, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415807d",
   "metadata": {},
   "source": [
    "Note that the type of pooling operator determines what kind of MP layer is used after pooling. \n",
    "A sparse pooler is followed by a sparse MP operator such as [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/2.6.1/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv). \n",
    "On the other hand, a dense pooling operator that returns a dense connectivity matrix must be followed by a dense MP layer such as [`DenseGCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.DenseGCNConv.html).\n",
    "The type of pooling operator can be checked by the property `is_dense`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee3d34",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Next, we define the forward pass of the GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, edge_index, edge_weight, batch=None):\n",
    "    # First MP layer\n",
    "    x = self.conv1(x, edge_index, edge_weight)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # Pooling\n",
    "    out = self.pooler(x=x, adj=edge_index, edge_weight=edge_weight, batch=batch)\n",
    "    x_pool, adj_pool = out.x, out.edge_index\n",
    "\n",
    "    # Second MP layer\n",
    "    x = self.conv2(x_pool, adj_pool)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # Global pooling\n",
    "    x = self.pooler.readout(x, reduce_op=\"sum\", batch=out.batch)\n",
    "\n",
    "    # Readout layer\n",
    "    x = self.lin(x)\n",
    "\n",
    "    if out.loss is not None:\n",
    "        return F.log_softmax(x, dim=-1), sum(out.get_loss_value())\n",
    "    else:\n",
    "        return F.log_softmax(x, dim=-1), torch.tensor(0.0)\n",
    "\n",
    "\n",
    "GNN.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3358d75",
   "metadata": {},
   "source": [
    "There are a few things to discuss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6cb40",
   "metadata": {},
   "source": [
    "#### Global pooling\n",
    "The global pooling operation combines all the features in the current graph and is implemented differently depending if the pooler is sparse or dense.\n",
    "In the sparse case, we have a `batch` tensor indicating to which graph each node belongs to. \n",
    "In this case, global pooling should combine the features of the nodes belonging to the same graph. \n",
    "The output is a tensor of shape $[B, F]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "055cff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1051, 3])\n",
      "Output shape: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Sparse case\n",
    "print(f\"Input shape: {data_batch.x.shape}\")\n",
    "out_global_sparse = pooler.readout(\n",
    "    data_batch.x, reduce_op=\"sum\", batch=data_batch.batch\n",
    ")\n",
    "print(f\"Output shape: {out_global_sparse.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf0a76",
   "metadata": {},
   "source": [
    "In the dense case, the features of the pooled graph are stored in a tensor of shape $[B, K, F]$ and global pooling can be done e.g., by summing or taking the average across the nodes dimension, yielding a tensor of shape $[B, F]$. In this case, `batch` is not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "723065eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 3])\n",
      "Output shape: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Dense case\n",
    "print(f\"Input shape: {dense_pooling_output.x.shape}\")\n",
    "out_global_dense = dense_pooler.readout(dense_pooling_output.x, reduce_op=\"sum\", batch=None)\n",
    "print(f\"Output shape: {out_global_dense.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a8cdf",
   "metadata": {},
   "source": [
    "Note that in both cases the output is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3057336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(out_global_sparse, out_global_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e370e6",
   "metadata": {},
   "source": [
    "#### Auxiliary losses\n",
    "As we saw earlier, some pooling operators return an auxiliary loss, while others do not.\n",
    "In the forward pass we check if `out.loss` is not `None` and, in case, return the sum of all the auxiliary losses to be passed to the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ad67f",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Let's first test our GNN when configured with a sparse pooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103ee50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse GNN output shape: torch.Size([32, 6])\n",
      "Sparse GNN loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "sparse_params = {\n",
    "    \"ratio\": 0.25,  # Percentage of nodes to keep\n",
    "}\n",
    "\n",
    "sparse_pool_gnn = GNN(\n",
    "    in_channels=num_features,\n",
    "    out_channels=num_classes,\n",
    "    pooler_type=\"topk\",\n",
    "    pooler_kwargs=sparse_params,\n",
    ")\n",
    "\n",
    "sparse_gnn_out = sparse_pool_gnn(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(f\"Sparse GNN output shape: {sparse_gnn_out[0].shape}\")\n",
    "print(f\"Sparse GNN loss: {sparse_gnn_out[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358d3da",
   "metadata": {},
   "source": [
    "Since there is no auxiliary loss, the second output of the GNN is simply a constant zero-valued tensor that will not affect the gradients computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d241a54",
   "metadata": {},
   "source": [
    "Next, we create the GNN with the dense pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d8ac867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense GNN output shape: torch.Size([32, 6])\n",
      "Dense GNN loss: 0.170\n"
     ]
    }
   ],
   "source": [
    "dense_params = {\n",
    "    \"k\": 10,  # Number of supernodes in the pooled graph\n",
    "}\n",
    "dense_pool_gnn = GNN(\n",
    "    in_channels=num_features,\n",
    "    out_channels=num_classes,\n",
    "    pooler_type=\"mincut\",\n",
    "    pooler_kwargs=dense_params,\n",
    ")\n",
    "dense_gnn_out = dense_pool_gnn(\n",
    "    x=data_batch.x,\n",
    "    edge_index=data_batch.edge_index,\n",
    "    edge_weight=data_batch.edge_weight,\n",
    "    batch=data_batch.batch,\n",
    ")\n",
    "print(f\"Dense GNN output shape: {dense_gnn_out[0].shape}\")\n",
    "print(f\"Dense GNN loss: {dense_gnn_out[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f379c1",
   "metadata": {},
   "source": [
    "This time, we get an auxiliary loss that should be added to the other losses, e.g. the classification loss of the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ed0c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.867\n"
     ]
    }
   ],
   "source": [
    "total_loss = F.nll_loss(dense_gnn_out[0], data_batch.y.view(-1)) + dense_gnn_out[1]\n",
    "print(f\"Loss: {total_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469ede0",
   "metadata": {},
   "source": [
    "And that's it! We can train this GNN as any other that we normally build with <img src=\"https://raw.githubusercontent.com/TorchSpatiotemporal/tsl/main/docs/source/_static/img/logos/pyg.svg\" width=\"20px\" align=\"center\"/> PyG.\n",
    "\n",
    "You can check the complete graph classification example [here](https://github.com/tgp-team/torch-geometric-pool/blob/main/examples/classification.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
